{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02fu_hQ_8hDO",
        "outputId": "d88f69e9-0662-4967-8bb2-8cdd7761c4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 32, 32]           2,592\n",
            "       BatchNorm2d-2           [-1, 96, 32, 32]             192\n",
            "         LeakyReLU-3           [-1, 96, 32, 32]               0\n",
            "            Conv2d-4          [-1, 124, 32, 32]          11,904\n",
            "       BatchNorm2d-5          [-1, 124, 32, 32]             248\n",
            "            Conv2d-6           [-1, 62, 32, 32]          53,568\n",
            "       BatchNorm2d-7           [-1, 62, 32, 32]             124\n",
            "         LeakyReLU-8           [-1, 62, 32, 32]               0\n",
            "            Conv2d-9           [-1, 62, 32, 32]          34,596\n",
            "      BatchNorm2d-10           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-11           [-1, 62, 32, 32]               0\n",
            "           Conv2d-12          [-1, 124, 32, 32]          69,192\n",
            "      BatchNorm2d-13          [-1, 124, 32, 32]             248\n",
            "AdaptiveAvgPool2d-14            [-1, 124, 1, 1]               0\n",
            "           Linear-15                    [-1, 7]             868\n",
            "             ReLU-16                    [-1, 7]               0\n",
            "           Linear-17                  [-1, 124]             868\n",
            "          Sigmoid-18                  [-1, 124]               0\n",
            "          SEBlock-19          [-1, 124, 32, 32]               0\n",
            "        Dropout2d-20          [-1, 124, 32, 32]               0\n",
            "        LeakyReLU-21          [-1, 124, 32, 32]               0\n",
            "    ResidualBlock-22          [-1, 124, 32, 32]               0\n",
            "           Conv2d-23           [-1, 62, 32, 32]          69,192\n",
            "      BatchNorm2d-24           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-25           [-1, 62, 32, 32]               0\n",
            "           Conv2d-26           [-1, 62, 32, 32]          34,596\n",
            "      BatchNorm2d-27           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-28           [-1, 62, 32, 32]               0\n",
            "           Conv2d-29          [-1, 124, 32, 32]          69,192\n",
            "      BatchNorm2d-30          [-1, 124, 32, 32]             248\n",
            "AdaptiveAvgPool2d-31            [-1, 124, 1, 1]               0\n",
            "           Linear-32                    [-1, 7]             868\n",
            "             ReLU-33                    [-1, 7]               0\n",
            "           Linear-34                  [-1, 124]             868\n",
            "          Sigmoid-35                  [-1, 124]               0\n",
            "          SEBlock-36          [-1, 124, 32, 32]               0\n",
            "        Dropout2d-37          [-1, 124, 32, 32]               0\n",
            "        LeakyReLU-38          [-1, 124, 32, 32]               0\n",
            "    ResidualBlock-39          [-1, 124, 32, 32]               0\n",
            "           Conv2d-40           [-1, 62, 32, 32]          69,192\n",
            "      BatchNorm2d-41           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-42           [-1, 62, 32, 32]               0\n",
            "           Conv2d-43           [-1, 62, 32, 32]          34,596\n",
            "      BatchNorm2d-44           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-45           [-1, 62, 32, 32]               0\n",
            "           Conv2d-46          [-1, 124, 32, 32]          69,192\n",
            "      BatchNorm2d-47          [-1, 124, 32, 32]             248\n",
            "AdaptiveAvgPool2d-48            [-1, 124, 1, 1]               0\n",
            "           Linear-49                    [-1, 7]             868\n",
            "             ReLU-50                    [-1, 7]               0\n",
            "           Linear-51                  [-1, 124]             868\n",
            "          Sigmoid-52                  [-1, 124]               0\n",
            "          SEBlock-53          [-1, 124, 32, 32]               0\n",
            "        Dropout2d-54          [-1, 124, 32, 32]               0\n",
            "        LeakyReLU-55          [-1, 124, 32, 32]               0\n",
            "    ResidualBlock-56          [-1, 124, 32, 32]               0\n",
            "           Conv2d-57           [-1, 62, 32, 32]          69,192\n",
            "      BatchNorm2d-58           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-59           [-1, 62, 32, 32]               0\n",
            "           Conv2d-60           [-1, 62, 32, 32]          34,596\n",
            "      BatchNorm2d-61           [-1, 62, 32, 32]             124\n",
            "        LeakyReLU-62           [-1, 62, 32, 32]               0\n",
            "           Conv2d-63          [-1, 124, 32, 32]          69,192\n",
            "      BatchNorm2d-64          [-1, 124, 32, 32]             248\n",
            "AdaptiveAvgPool2d-65            [-1, 124, 1, 1]               0\n",
            "           Linear-66                    [-1, 7]             868\n",
            "             ReLU-67                    [-1, 7]               0\n",
            "           Linear-68                  [-1, 124]             868\n",
            "          Sigmoid-69                  [-1, 124]               0\n",
            "          SEBlock-70          [-1, 124, 32, 32]               0\n",
            "        Dropout2d-71          [-1, 124, 32, 32]               0\n",
            "        LeakyReLU-72          [-1, 124, 32, 32]               0\n",
            "    ResidualBlock-73          [-1, 124, 32, 32]               0\n",
            "           Conv2d-74          [-1, 189, 16, 16]          23,436\n",
            "      BatchNorm2d-75          [-1, 189, 16, 16]             378\n",
            "           Conv2d-76           [-1, 94, 32, 32]         104,904\n",
            "      BatchNorm2d-77           [-1, 94, 32, 32]             188\n",
            "        LeakyReLU-78           [-1, 94, 32, 32]               0\n",
            "           Conv2d-79           [-1, 94, 16, 16]          79,524\n",
            "      BatchNorm2d-80           [-1, 94, 16, 16]             188\n",
            "        LeakyReLU-81           [-1, 94, 16, 16]               0\n",
            "           Conv2d-82          [-1, 189, 16, 16]         159,894\n",
            "      BatchNorm2d-83          [-1, 189, 16, 16]             378\n",
            "AdaptiveAvgPool2d-84            [-1, 189, 1, 1]               0\n",
            "           Linear-85                   [-1, 11]           2,079\n",
            "             ReLU-86                   [-1, 11]               0\n",
            "           Linear-87                  [-1, 189]           2,079\n",
            "          Sigmoid-88                  [-1, 189]               0\n",
            "          SEBlock-89          [-1, 189, 16, 16]               0\n",
            "        Dropout2d-90          [-1, 189, 16, 16]               0\n",
            "        LeakyReLU-91          [-1, 189, 16, 16]               0\n",
            "    ResidualBlock-92          [-1, 189, 16, 16]               0\n",
            "           Conv2d-93           [-1, 94, 16, 16]         159,894\n",
            "      BatchNorm2d-94           [-1, 94, 16, 16]             188\n",
            "        LeakyReLU-95           [-1, 94, 16, 16]               0\n",
            "           Conv2d-96           [-1, 94, 16, 16]          79,524\n",
            "      BatchNorm2d-97           [-1, 94, 16, 16]             188\n",
            "        LeakyReLU-98           [-1, 94, 16, 16]               0\n",
            "           Conv2d-99          [-1, 189, 16, 16]         159,894\n",
            "     BatchNorm2d-100          [-1, 189, 16, 16]             378\n",
            "AdaptiveAvgPool2d-101            [-1, 189, 1, 1]               0\n",
            "          Linear-102                   [-1, 11]           2,079\n",
            "            ReLU-103                   [-1, 11]               0\n",
            "          Linear-104                  [-1, 189]           2,079\n",
            "         Sigmoid-105                  [-1, 189]               0\n",
            "         SEBlock-106          [-1, 189, 16, 16]               0\n",
            "       Dropout2d-107          [-1, 189, 16, 16]               0\n",
            "       LeakyReLU-108          [-1, 189, 16, 16]               0\n",
            "   ResidualBlock-109          [-1, 189, 16, 16]               0\n",
            "          Conv2d-110           [-1, 94, 16, 16]         159,894\n",
            "     BatchNorm2d-111           [-1, 94, 16, 16]             188\n",
            "       LeakyReLU-112           [-1, 94, 16, 16]               0\n",
            "          Conv2d-113           [-1, 94, 16, 16]          79,524\n",
            "     BatchNorm2d-114           [-1, 94, 16, 16]             188\n",
            "       LeakyReLU-115           [-1, 94, 16, 16]               0\n",
            "          Conv2d-116          [-1, 189, 16, 16]         159,894\n",
            "     BatchNorm2d-117          [-1, 189, 16, 16]             378\n",
            "AdaptiveAvgPool2d-118            [-1, 189, 1, 1]               0\n",
            "          Linear-119                   [-1, 11]           2,079\n",
            "            ReLU-120                   [-1, 11]               0\n",
            "          Linear-121                  [-1, 189]           2,079\n",
            "         Sigmoid-122                  [-1, 189]               0\n",
            "         SEBlock-123          [-1, 189, 16, 16]               0\n",
            "       Dropout2d-124          [-1, 189, 16, 16]               0\n",
            "       LeakyReLU-125          [-1, 189, 16, 16]               0\n",
            "   ResidualBlock-126          [-1, 189, 16, 16]               0\n",
            "          Conv2d-127           [-1, 94, 16, 16]         159,894\n",
            "     BatchNorm2d-128           [-1, 94, 16, 16]             188\n",
            "       LeakyReLU-129           [-1, 94, 16, 16]               0\n",
            "          Conv2d-130           [-1, 94, 16, 16]          79,524\n",
            "     BatchNorm2d-131           [-1, 94, 16, 16]             188\n",
            "       LeakyReLU-132           [-1, 94, 16, 16]               0\n",
            "          Conv2d-133          [-1, 189, 16, 16]         159,894\n",
            "     BatchNorm2d-134          [-1, 189, 16, 16]             378\n",
            "AdaptiveAvgPool2d-135            [-1, 189, 1, 1]               0\n",
            "          Linear-136                   [-1, 11]           2,079\n",
            "            ReLU-137                   [-1, 11]               0\n",
            "          Linear-138                  [-1, 189]           2,079\n",
            "         Sigmoid-139                  [-1, 189]               0\n",
            "         SEBlock-140          [-1, 189, 16, 16]               0\n",
            "       Dropout2d-141          [-1, 189, 16, 16]               0\n",
            "       LeakyReLU-142          [-1, 189, 16, 16]               0\n",
            "   ResidualBlock-143          [-1, 189, 16, 16]               0\n",
            "          Conv2d-144            [-1, 280, 8, 8]          52,920\n",
            "     BatchNorm2d-145            [-1, 280, 8, 8]             560\n",
            "          Conv2d-146          [-1, 140, 16, 16]         238,140\n",
            "     BatchNorm2d-147          [-1, 140, 16, 16]             280\n",
            "       LeakyReLU-148          [-1, 140, 16, 16]               0\n",
            "          Conv2d-149            [-1, 140, 8, 8]         176,400\n",
            "     BatchNorm2d-150            [-1, 140, 8, 8]             280\n",
            "       LeakyReLU-151            [-1, 140, 8, 8]               0\n",
            "          Conv2d-152            [-1, 280, 8, 8]         352,800\n",
            "     BatchNorm2d-153            [-1, 280, 8, 8]             560\n",
            "AdaptiveAvgPool2d-154            [-1, 280, 1, 1]               0\n",
            "          Linear-155                   [-1, 17]           4,760\n",
            "            ReLU-156                   [-1, 17]               0\n",
            "          Linear-157                  [-1, 280]           4,760\n",
            "         Sigmoid-158                  [-1, 280]               0\n",
            "         SEBlock-159            [-1, 280, 8, 8]               0\n",
            "       Dropout2d-160            [-1, 280, 8, 8]               0\n",
            "       LeakyReLU-161            [-1, 280, 8, 8]               0\n",
            "   ResidualBlock-162            [-1, 280, 8, 8]               0\n",
            "          Conv2d-163            [-1, 140, 8, 8]         352,800\n",
            "     BatchNorm2d-164            [-1, 140, 8, 8]             280\n",
            "       LeakyReLU-165            [-1, 140, 8, 8]               0\n",
            "          Conv2d-166            [-1, 140, 8, 8]         176,400\n",
            "     BatchNorm2d-167            [-1, 140, 8, 8]             280\n",
            "       LeakyReLU-168            [-1, 140, 8, 8]               0\n",
            "          Conv2d-169            [-1, 280, 8, 8]         352,800\n",
            "     BatchNorm2d-170            [-1, 280, 8, 8]             560\n",
            "AdaptiveAvgPool2d-171            [-1, 280, 1, 1]               0\n",
            "          Linear-172                   [-1, 17]           4,760\n",
            "            ReLU-173                   [-1, 17]               0\n",
            "          Linear-174                  [-1, 280]           4,760\n",
            "         Sigmoid-175                  [-1, 280]               0\n",
            "         SEBlock-176            [-1, 280, 8, 8]               0\n",
            "       Dropout2d-177            [-1, 280, 8, 8]               0\n",
            "       LeakyReLU-178            [-1, 280, 8, 8]               0\n",
            "   ResidualBlock-179            [-1, 280, 8, 8]               0\n",
            "          Conv2d-180            [-1, 140, 8, 8]         352,800\n",
            "     BatchNorm2d-181            [-1, 140, 8, 8]             280\n",
            "       LeakyReLU-182            [-1, 140, 8, 8]               0\n",
            "          Conv2d-183            [-1, 140, 8, 8]         176,400\n",
            "     BatchNorm2d-184            [-1, 140, 8, 8]             280\n",
            "       LeakyReLU-185            [-1, 140, 8, 8]               0\n",
            "          Conv2d-186            [-1, 280, 8, 8]         352,800\n",
            "     BatchNorm2d-187            [-1, 280, 8, 8]             560\n",
            "AdaptiveAvgPool2d-188            [-1, 280, 1, 1]               0\n",
            "          Linear-189                   [-1, 17]           4,760\n",
            "            ReLU-190                   [-1, 17]               0\n",
            "          Linear-191                  [-1, 280]           4,760\n",
            "         Sigmoid-192                  [-1, 280]               0\n",
            "         SEBlock-193            [-1, 280, 8, 8]               0\n",
            "       Dropout2d-194            [-1, 280, 8, 8]               0\n",
            "       LeakyReLU-195            [-1, 280, 8, 8]               0\n",
            "   ResidualBlock-196            [-1, 280, 8, 8]               0\n",
            "AdaptiveAvgPool2d-197            [-1, 280, 1, 1]               0\n",
            "          Linear-198                   [-1, 10]           2,810\n",
            "================================================================\n",
            "Total params: 4,905,430\n",
            "Trainable params: 4,905,430\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 59.35\n",
            "Params size (MB): 18.71\n",
            "Estimated Total Size (MB): 78.08\n",
            "----------------------------------------------------------------\n",
            "Epoch 1, Train Loss: 1.7347, Train Acc: 35.57%, Val Loss: 1.5496, Val Acc: 43.44%\n",
            "Epoch 2, Train Loss: 1.3619, Train Acc: 50.57%, Val Loss: 1.6072, Val Acc: 48.97%\n",
            "Epoch 3, Train Loss: 1.1367, Train Acc: 59.27%, Val Loss: 0.9935, Val Acc: 64.07%\n",
            "Epoch 4, Train Loss: 0.9859, Train Acc: 64.60%, Val Loss: 1.0195, Val Acc: 65.30%\n",
            "Epoch 5, Train Loss: 0.8794, Train Acc: 68.92%, Val Loss: 0.8163, Val Acc: 71.73%\n",
            "Epoch 6, Train Loss: 0.7864, Train Acc: 72.26%, Val Loss: 0.9147, Val Acc: 69.89%\n",
            "Epoch 7, Train Loss: 0.7183, Train Acc: 74.93%, Val Loss: 0.6488, Val Acc: 78.05%\n",
            "Epoch 8, Train Loss: 0.6597, Train Acc: 76.85%, Val Loss: 0.6366, Val Acc: 78.80%\n",
            "Epoch 9, Train Loss: 0.6093, Train Acc: 78.68%, Val Loss: 0.6982, Val Acc: 78.55%\n",
            "Epoch 10, Train Loss: 0.5699, Train Acc: 80.19%, Val Loss: 0.5079, Val Acc: 82.56%\n",
            "Epoch 11, Train Loss: 0.5268, Train Acc: 81.80%, Val Loss: 0.5921, Val Acc: 79.78%\n",
            "Epoch 12, Train Loss: 0.5069, Train Acc: 82.30%, Val Loss: 0.4935, Val Acc: 83.75%\n",
            "Epoch 13, Train Loss: 0.4795, Train Acc: 83.35%, Val Loss: 0.4706, Val Acc: 84.35%\n",
            "Epoch 14, Train Loss: 0.4574, Train Acc: 84.01%, Val Loss: 0.4401, Val Acc: 84.99%\n",
            "Epoch 15, Train Loss: 0.4396, Train Acc: 84.66%, Val Loss: 0.5062, Val Acc: 83.45%\n",
            "Epoch 16, Train Loss: 0.4190, Train Acc: 85.49%, Val Loss: 0.4534, Val Acc: 84.94%\n",
            "Epoch 17, Train Loss: 0.4001, Train Acc: 86.11%, Val Loss: 0.4201, Val Acc: 85.66%\n",
            "Epoch 18, Train Loss: 0.3845, Train Acc: 86.66%, Val Loss: 0.3854, Val Acc: 87.20%\n",
            "Epoch 19, Train Loss: 0.3761, Train Acc: 86.99%, Val Loss: 0.3820, Val Acc: 87.57%\n",
            "Epoch 20, Train Loss: 0.3566, Train Acc: 87.57%, Val Loss: 0.3611, Val Acc: 88.02%\n",
            "Epoch 21, Train Loss: 0.3522, Train Acc: 87.68%, Val Loss: 0.3859, Val Acc: 87.56%\n",
            "Epoch 22, Train Loss: 0.3376, Train Acc: 88.39%, Val Loss: 0.3511, Val Acc: 87.82%\n",
            "Epoch 23, Train Loss: 0.3277, Train Acc: 88.64%, Val Loss: 0.3296, Val Acc: 88.91%\n",
            "Epoch 24, Train Loss: 0.3241, Train Acc: 88.76%, Val Loss: 0.3690, Val Acc: 87.84%\n",
            "Epoch 25, Train Loss: 0.3107, Train Acc: 89.23%, Val Loss: 0.3505, Val Acc: 88.82%\n",
            "Epoch 26, Train Loss: 0.2997, Train Acc: 89.62%, Val Loss: 0.3677, Val Acc: 87.97%\n",
            "Epoch 27, Train Loss: 0.2950, Train Acc: 89.84%, Val Loss: 0.3638, Val Acc: 88.20%\n",
            "Epoch 28, Train Loss: 0.2867, Train Acc: 90.00%, Val Loss: 0.3716, Val Acc: 88.09%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "from PIL import Image\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# auto. choose CPU or GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Function to load CIFAR-10 dataset\n",
        "def load_cifar_batch(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# Specify the directory containing CIFAR-10 batches\n",
        "cifar10_dir = '/content/'\n",
        "# Load metadata (labels)\n",
        "meta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))\n",
        "label_names = [label.decode('utf-8') for label in meta_data_dict[b'label_names']]\n",
        "\n",
        "# Load training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "for i in range(1, 6):\n",
        "    batch = load_cifar_batch(os.path.join(cifar10_dir, f'data_batch_{i}'))\n",
        "    train_data.append(batch[b'data'])\n",
        "    train_labels += batch[b'labels']\n",
        "\n",
        "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Convert to HWC format\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness = 0.1,contrast = 0.1,saturation = 0.1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor = 2,p = 0.2),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    transforms.RandomErasing(p=0.2,scale=(0.02, 0.1),value=1.0, inplace=False)\n",
        "])\n",
        "\n",
        "# Convert to TensorDataset and apply transformations\n",
        "class CustomCIFAR10Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "train_dataset = CustomCIFAR10Dataset(train_data, train_labels, transform=transform)\n",
        "\n",
        "# Split into training and validation sets\n",
        "#train_size = int(0.9 * len(train_dataset))\n",
        "#val_size = len(train_dataset) - train_size\n",
        "#train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "batch_test_dict = load_cifar_batch(os.path.join(cifar10_dir, 'test_batch'))\n",
        "val_images = batch_test_dict[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "val_labels = np.array(batch_test_dict[b'labels'])\n",
        "\n",
        "val_dataset = CustomCIFAR10Dataset(val_images, val_labels, transform=test_transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "# Load test dataset\n",
        "cifar_test_path = '/content/cifar_test_nolabel.pkl'\n",
        "test_batch = load_cifar_batch(cifar_test_path)\n",
        "test_images = test_batch[b'data'].astype(np.float32) / 255.0\n",
        "\n",
        "# Convert test dataset to Tensor\n",
        "test_dataset = [(test_transform(img),) for img in test_images]\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "# Train function + plot\n",
        "def train_model(model, train_loader, val_loader, epochs=50):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80, 90], gamma=0.1)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Plot Losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', color='red')\n",
        "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', color='blue')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train Loss & Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Accuracies\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy', color='green')\n",
        "    plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', color='purple')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Train Accuracy & Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the Squeeze-and-Excitation (SE) Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.global_avg_pool(x).view(b, c)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# Define Residual Block with Shortcut Connections\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_sizes, stride=1, use_se=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        mid_channels = out_channels // 2\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=kernel_sizes[0], padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=kernel_sizes[1], padding=1, stride=stride, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=kernel_sizes[2], padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.se = SEBlock(out_channels) if use_se else nn.Identity()\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        out = self.leaky_relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out = self.se(out)\n",
        "        out = self.dropout(out)\n",
        "        out += identity\n",
        "        return self.leaky_relu(out)\n",
        "\n",
        "class CustomResNet_v4(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomResNet_v4, self).__init__()\n",
        "        # Initial convolution layer with a small increase in channels\n",
        "        self.init_conv = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.init_bn = nn.BatchNorm2d(96)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "        # First residual block with slightly increased channels\n",
        "        self.layer1 = self._make_layer(96, 124, [3, 3, 3], 4, stride=1)\n",
        "        # Second residual block with slightly increased channels\n",
        "        self.layer2 = self._make_layer(124, 189, [3, 3, 3], 4, stride=2)\n",
        "        # Third residual block with slightly increased channels\n",
        "        self.layer3 = self._make_layer(189, 280, [3, 3, 3], 3, stride=2)\n",
        "\n",
        "        # Final average pooling and fully connected layers\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(280, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, kernel_sizes, blocks, stride):\n",
        "        layers = [ResidualBlock(in_channels, out_channels, kernel_sizes, stride)]\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, kernel_sizes, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.leaky_relu(self.init_bn(self.init_conv(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = CustomResNet_v4().to(device)\n",
        "\n",
        "# Define the optimizer, scheduler, and loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Print the model summary\n",
        "from torchsummary import summary\n",
        "summary(model, (3, 32, 32))\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, epochs=70) #change epoch\n",
        "\n",
        "# Generate submission file\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images = batch[0].to(device)  # Get images tensor from tuple and move to device\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate submission file\n",
        "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
        "submission.to_csv('/content/submission1.csv', index=False)\n",
        "print(\"Submission1 file saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOE3jvRm-kd7"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/model_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W7aNviqaqqG"
      },
      "outputs": [],
      "source": [
        "# Generate submission file\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images = batch[0].to(device)  # Get images tensor from tuple and move to device\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate submission file\n",
        "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
        "submission.to_csv('/content/submission2.csv', index=False)\n",
        "print(\"Submission1 file saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq0M7nEJa3LO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}