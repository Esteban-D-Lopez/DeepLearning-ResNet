{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically explore potential optimizations for the ResNet architecture while staying within the 5 million parameter constraint we will experiment with a variety of hyperparameters across architecture, training, optimization, and regularization settings**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Architecture Hyperparameters\n",
    "\n",
    "| **Hyperparameter** | **Description** | **Possible Values to Test** |\n",
    "|-------------------|----------------|---------------------------|\n",
    "| **Number of Residual Blocks** | Controls model depth and complexity | 8, 10, 14 |\n",
    "| **Number of Channels per Layer** | Determines feature map size per block | 16, 32, 64 |\n",
    "| **Kernel Size (Standard Conv)** | Controls receptive field per layer | 3x3, 5x5 |\n",
    "| **Kernel Size (Skip Connection)** | Controls information flow in residual block | 1x1, 3x3 |\n",
    "| **Use Depthwise Separable Convs** | Reduces parameter count while maintaining expressiveness | Yes, No |\n",
    "| **Use Squeeze-and-Excitation Blocks** | Enhances channel-wise feature recalibration | Yes, No |\n",
    "| **Global Average Pooling (GAP) vs. Fully Connected Layers** | Reduces overfitting and parameter count | GAP, FC Layers |\n",
    "| **DropPath Regularization** | Prevents overfitting in deeper models | 0.1, 0.2, 0.3 |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Training Hyperparameters\n",
    "\n",
    "| **Hyperparameter** | **Description** | **Possible Values to Test** |\n",
    "|-------------------|----------------|---------------------------|\n",
    "| **Batch Size** | Controls the number of samples per update | 64, 128, 256 |\n",
    "| **Learning Rate (Initial)** | Defines step size for weight updates | 0.01, 0.001, 0.0005 |\n",
    "| **Optimizer Type** | Determines how gradients are updated | SGD (Momentum), AdamW, RMSProp, LAMB |\n",
    "| **Momentum (for SGD)** | Helps smooth gradient updates | 0.8, 0.9, 0.95 |\n",
    "| **Weight Decay (L2 Regularization)** | Prevents overfitting by penalizing large weights | 1e-5, 1e-4, 1e-3 |\n",
    "| **Gradient Clipping** | Prevents exploding gradients | No Clipping, Max Norm (1.0, 5.0) |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Learning Rate Scheduling Strategies\n",
    "\n",
    "| **Scheduler Type** | **Description** | **Hyperparameters to Tune** |\n",
    "|-------------------|----------------|---------------------------|\n",
    "| **Cosine Annealing** | Smooth decay of learning rate | Min LR: 1e-6, Restart Intervals |\n",
    "| **Step Decay** | Reduces LR at fixed intervals | Drop Rate: 0.1, Step Size: 10 epochs |\n",
    "| **OneCycleLR** | Uses a cyclic LR for fast convergence | Max LR: 0.1, Anneal Strategy |\n",
    "| **Cyclic LR** | Periodically increases and decreases LR | Base LR: 1e-4, Max LR: 1e-2 |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Augmentation Hyperparameters\n",
    "\n",
    "| **Augmentation Type** | **Description** | **Possible Values to Test** |\n",
    "|----------------------|----------------|---------------------------|\n",
    "| **CutMix** | Replaces a patch of an image with another image | Alpha = 0.2, 0.4 |\n",
    "| **MixUp** | Blends two images with a weighted sum | Alpha = 0.1, 0.2, 0.4 |\n",
    "| **AutoAugment** | Uses a learned augmentation policy | CIFAR-10 Policy, SVHN Policy |\n",
    "| **Random Erasing** | Randomly removes parts of an image | Probability = 0.25, 0.5 |\n",
    "| **Horizontal Flip** | Flips images left-right | Always On, 50% Probability |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Batch Normalization & Dropout Hyperparameters\n",
    "\n",
    "| **Regularization Type** | **Description** | **Possible Values to Test** |\n",
    "|----------------------|----------------|---------------------------|\n",
    "| **BatchNorm Momentum** | Affects stability of normalization layers | 0.8, 0.9, 0.99 |\n",
    "| **Dropout Rate** | Prevents overfitting by randomly disabling neurons | 0.2, 0.3, 0.5 |\n",
    "| **Label Smoothing** | Reduces confidence of softmax predictions | 0.05, 0.1, 0.2 |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Residual Block Optimizations\n",
    "\n",
    "| **Block Design Choice** | **Description** | **Possible Values to Test** |\n",
    "|------------------------|----------------|---------------------------|\n",
    "| **Number of Residual Blocks** | Controls model depth | 3, 5, 7 |\n",
    "| **Use Grouped Convolutions** | Splits filters into smaller groups | Yes, No |\n",
    "| **Position of BatchNorm in Residual Block** | Affects gradient flow and stability | Before Activation, After Activation |\n",
    "| **Shortcut Connection Type** | Defines how information flows through blocks | Identity Shortcut, 1x1 Conv |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Model Selection Criteria\n",
    "Compare and integrate the best-performing:\n",
    "- **Optimizer + Learning Rate Schedule**\n",
    "- **Data Augmentation Strategy**\n",
    "- **Residual Block Architecture**\n",
    "- **Regularization Techniques**\n",
    "- **Batch Normalization & Dropout Settings**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
